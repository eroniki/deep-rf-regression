{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import numpy as np\n",
      "import wifi_data\n",
      "import os\n",
      "import nn\n",
      "\n",
      "def main():\n",
      "    fname_dataset = \"hancock_data.mat\"\n",
      "    fname_model = \"grid_classifier.h5\"\n",
      "    folderLocation = os.path.dirname(os.path.realpath(__file__))\n",
      "    # Create the dataset\n",
      "    dataset = wifi_data.wifi_data(folder_location=folderLocation, filename=fname_dataset, normalize=False, missingValues=0, nTraining=1200, nTesting=343, nValidation=0, verbose=False)\n",
      "    # Show the dataset properties\n",
      "    print \"Training Shape: \", dataset.train_set.shape, dataset.pos_train.shape, dataset.pos_train_classifier.shape\n",
      "    print \"Test Shape: \", dataset.test_set.shape, dataset.pos_test.shape, dataset.pos_test_classifier.shape\n",
      "    # Validation split can be omitted, because Keras can create a validation\n",
      "    # split from the training set\n",
      "    if dataset.valid_set != None:\n",
      "        print \"Validation Shape: \", dataset.valid_set.shape, dataset.pos_valid.shape\n",
      "\n",
      "    # Initiate the Neural Network Object\n",
      "    myNN = nn.nn(model_name=fname_model, location=folderLocation, valid_split = 0.15, epoch=10000)\n",
      "    # myGRU = gru.gru(model_name=fname_model_rnn, location=folderLocation, valid_split = 0.2, epoch=100)\n",
      "    lora_only = dataset.train_set[:,-8:];\n",
      "    # Check if the model already exists, which prevents re-training\n",
      "    # If the model does NOT exist, train the model with the training samples\n",
      "    # Otherwise, the already existing model will be used.\n",
      "    if not myNN.model_exists:\n",
      "        myNN.fit_model(lora_only, dataset.pos_train_classifier)\n",
      "        # myGRU.fit_model(dataset.train_set, dataset.pos_train)\n",
      "    # # Testing\n",
      "    # # loc_hat = myNN.predict(dataset.test_set)\n",
      "    # # loc_hat = myGRU.predict(dataset.test_set)\n",
      "    print myNN.model.evaluate(lora_only, dataset.pos_train_classifier, batch_size=32, verbose=1, sample_weight=None)\n",
      "\n",
      "    # Evaluation\n",
      "    # mse, mse_x, mse_y = myNN.residual_analysis(y=dataset.pos_test, y_hat=loc_hat, plot_cdf=True, nbins=100)\n",
      "    # mse, mse_x, mse_y = myGRU.residual_analysis(y=dataset.pos_test, y_hat=loc_hat, plot_cdf=True, nbins=100)\n",
      "    #\n",
      "    # print \"\\nMSE: \", mse, \"MSE_x: \", mse_x, \" MSE_y: \", mse_y\n",
      "    #\n",
      "    # answer = raw_input(\"Would you like to save the model? Y/n\\n\")\n",
      "    # if answer[0] == 'y' or answer[0] == 'Y':\n",
      "    #     myNN.save_model(folderLocation, fname_model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using TensorFlow backend.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name '__file__' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-1-3cf0a5785d6f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfname_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hancock_data.mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfname_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"grid_classifier.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfolderLocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Create the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwifi_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwifi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolderLocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissingValues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnTraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnTesting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m343\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnValidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name '__file__' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import numpy as np\n",
      "import wifi_data\n",
      "import os\n",
      "import nn\n",
      "\n",
      "def main():\n",
      "    fname_dataset = \"hancock_data.mat\"\n",
      "    fname_model = \"grid_classifier.h5\"\n",
      "    folderLocation = os.path.dirname(os.path.realpath(__file__))\n",
      "    # Create the dataset\n",
      "    dataset = wifi_data.wifi_data(folder_location=folderLocation, filename=fname_dataset, normalize=False, missingValues=0, nTraining=1200, nTesting=343, nValidation=0, verbose=False)\n",
      "    # Show the dataset properties\n",
      "    print \"Training Shape: \", dataset.train_set.shape, dataset.pos_train.shape, dataset.pos_train_classifier.shape\n",
      "    print \"Test Shape: \", dataset.test_set.shape, dataset.pos_test.shape, dataset.pos_test_classifier.shape\n",
      "    # Validation split can be omitted, because Keras can create a validation\n",
      "    # split from the training set\n",
      "    if dataset.valid_set != None:\n",
      "        print \"Validation Shape: \", dataset.valid_set.shape, dataset.pos_valid.shape\n",
      "\n",
      "    # Initiate the Neural Network Object\n",
      "    myNN = nn.nn(model_name=fname_model, location=folderLocation, valid_split = 0.15, epoch=10000)\n",
      "    # myGRU = gru.gru(model_name=fname_model_rnn, location=folderLocation, valid_split = 0.2, epoch=100)\n",
      "    lora_only = dataset.train_set[:,-8:];\n",
      "    # Check if the model already exists, which prevents re-training\n",
      "    # If the model does NOT exist, train the model with the training samples\n",
      "    # Otherwise, the already existing model will be used.\n",
      "    if not myNN.model_exists:\n",
      "        myNN.fit_model(lora_only, dataset.pos_train_classifier)\n",
      "        # myGRU.fit_model(dataset.train_set, dataset.pos_train)\n",
      "    # # Testing\n",
      "    # # loc_hat = myNN.predict(dataset.test_set)\n",
      "    # # loc_hat = myGRU.predict(dataset.test_set)\n",
      "    print myNN.model.evaluate(lora_only, dataset.pos_train_classifier, batch_size=32, verbose=1, sample_weight=None)\n",
      "\n",
      "    # Evaluation\n",
      "    # mse, mse_x, mse_y = myNN.residual_analysis(y=dataset.pos_test, y_hat=loc_hat, plot_cdf=True, nbins=100)\n",
      "    # mse, mse_x, mse_y = myGRU.residual_analysis(y=dataset.pos_test, y_hat=loc_hat, plot_cdf=True, nbins=100)\n",
      "    #\n",
      "    # print \"\\nMSE: \", mse, \"MSE_x: \", mse_x, \" MSE_y: \", mse_y\n",
      "    #\n",
      "    # answer = raw_input(\"Would you like to save the model? Y/n\\n\")\n",
      "    # if answer[0] == 'y' or answer[0] == 'Y':\n",
      "    #     myNN.save_model(folderLocation, fname_model)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}